<!DOCTYPE html>
<html>
    <head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
  <meta name="robots" content="index, follow">
  <!-- title -->
  
    
  <title>《这就是 ChatGPT 》 读书笔记</title>
    
  
  
  <!-- open graph -->
  <meta name="description" content="通读整本书，作者认为 ChatGPT 能生成有意义的文本，也像是一种玄学。比较简单的模型还能用科学来解释，GPT 这样复杂的大模型已经无法用科学来解释了。（这是我个人的理解） 大佬写的书特别通俗易懂，看完能大概了解 ChatGPT 的原理。下面是我的一部分摘抄。 ChatGPT 在做什么 想象一下浏览人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后看看接下来出现的是什">
<meta property="og:type" content="article">
<meta property="og:title" content="《这就是 ChatGPT 》 读书笔记">
<meta property="og:url" content="https://fzero17.com/2024/04/26/what-is-chatgpt-doing-and-why-does-it-work/index.html">
<meta property="og:site_name" content="fzero 的日常记录">
<meta property="og:description" content="通读整本书，作者认为 ChatGPT 能生成有意义的文本，也像是一种玄学。比较简单的模型还能用科学来解释，GPT 这样复杂的大模型已经无法用科学来解释了。（这是我个人的理解） 大佬写的书特别通俗易懂，看完能大概了解 ChatGPT 的原理。下面是我的一部分摘抄。 ChatGPT 在做什么 想象一下浏览人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后看看接下来出现的是什">
<meta property="og:locale">
<meta property="article:published_time" content="2024-04-26T01:31:15.000Z">
<meta property="article:modified_time" content="2024-12-24T02:57:10.766Z">
<meta property="article:author" content="Fzero">
<meta property="article:tag" content="阅读">
<meta name="twitter:card" content="summary">
  <!-- canonical -->
  
  <link rel="canonical" href="https://fzero17.com/2024/04/26/what-is-chatgpt-doing-and-why-does-it-work/">
  
  <!-- Favicon -->
  <link rel="shortcut icon" href="">
  <link rel="apple-touch-icon" sizes="180x180" href="/img/apple-touch-icon.png">
  <!-- CSS -->
  
<link rel="stylesheet" href="/css/reset.css">

  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/css/markdown.css">

  
<link rel="stylesheet" href="/css/fonts.css">

<meta name="generator" content="Hexo 7.1.1"></head>

    <body>
        <div class="paper">
            <div class="paper-main">
                
                    <div class="post-header">
    <a class="logo" href="/">fzero 的日常记录</a>
    <!-- <div class="logo"><a href="/" title="Len"><img src="/img/logo.svg" alt="Len" aria-label="logo" height="20"></a></div> -->
        <ul class="nav">
            
            <li><a href="/">Home</a></li>
            
            <li><a href="https://fzero17.com/webmemo">WebMemo</a></li>
            
            <li><a href="https://fzero17.com/running_page">Running</a></li>
            
        </ul>


    </a>
</div>

                
                <div class="post-main">
    
        <div class="post-main-title">
            《这就是 ChatGPT 》 读书笔记
        </div>
        <div class="post-meta">
            2024-04-26
                <!-- 统计字数 -->
                <span>｜ 字数：1938</span>｜
                
        </div>
        <!-- 圆角分类 -->
        <!-- <div class="tags"> -->
        <!--  -->
        <!-- </div> -->
        <div class="post-md">
            <p>通读整本书，作者认为 ChatGPT 能生成有意义的文本，也像是一种玄学。比较简单的模型还能用科学来解释，GPT 这样复杂的大模型已经无法用科学来解释了。（这是我个人的理解）</p>
<p>大佬写的书特别通俗易懂，看完能大概了解 ChatGPT 的原理。下面是我的一部分摘抄。</p>
<h3 id="ChatGPT-在做什么"><a href="#ChatGPT-在做什么" class="headerlink" title="ChatGPT 在做什么"></a>ChatGPT 在做什么</h3><ul>
<li><p>想象一下浏览人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后看看接下来出现的是什么词，以及这些词出现的概率是多少。ChatGPT 实际上做了类似的事情，只不过它不是查看字面上的文本，而是寻找在某种程度上“意义匹配”的事物。</p>
</li>
<li><p>当 ChatGPT 做一些事情，比如写一篇文章时，它实质上只是在一遍又一遍地询问“根据目前的文本，下一个词应该是什么”，并且每次都添加一个词。</p>
</li>
<li><p>温度（temperature)。 符合玄学思想的是，有一个所谓的“温度”参数来确定低排名词的使用频率。对于文章生成来说，“温度”为 0.8 似乎最好。（二八定律？）</p>
</li>
<li><p>ChatGPT 的原始输入是一个由数组成的数组（到目前为止标记的嵌入向量）。当 ChatGPT“运行”以产生新标记时，这些数就会“依次通过”神经网络的各层，而每个神经元都会“做好本职工作”并将结果传递给下一层的神经元。没有循环和“回顾”。一切都是在网络中“向前馈送”的。</p>
</li>
</ul>
<h3 id="一些名词"><a href="#一些名词" class="headerlink" title="一些名词"></a>一些名词</h3><blockquote>
<p>以下名词是我在阅读过程中有不理解的地方，通过 ChatGPT 了解的释义，仅供参考。</p>
</blockquote>
<ul>
<li><p>token：在 GPT（Generative Pre-trained Transformer）和其他自然语言处理（NLP）模型中，”token” 是指文本中的一个基本单位，用于模型的输入或输出。Token 可以是一个词、一个字符、或者更特定的文本片段，这取决于模型的设计和预处理步骤。在大多数现代 NLP 模型中，特别是在 GPT 这样的大型语言模型中，token 通常是通过一种称为 “subword tokenization” 的方法得到的，这种方法允许模型有效地处理未知词汇、减少词汇表的大小，并保持对词形变化的敏感性。在 GPT 和其他基于 Transformer 的模型中，tokenization 是模型能够理解和生成自然语言的基础。通过这种方式，模型能够学习到语言的复杂结构和语义，从而在各种任务上，如文本生成、翻译、摘要等，表现出卓越的性能。</p>
</li>
<li><p>temperature：在文本生成模型（如 GPT）的上下文中，temperature 是一个控制生成文本多样性的参数。它影响模型在选择下一个词时的随机性。temperature 参数通常设置在 0 到 1 之间，但也可以超过 1。<br>当 temperature 接近 0 时，模型倾向于选择概率最高的词，导致生成的文本非常确定性和重复性，但可能缺乏多样性和创造性。<br>当 temperature 设置为 1 时，模型按照词的概率分布来选择下一个词，保持原有的多样性。<br>当 temperature 大于 1 时，会增加生成文本的随机性和多样性，但同时也可能降低文本的连贯性和可读性。<br>通过调整 temperature 参数，可以在生成的文本的多样性和连贯性之间找到一个平衡点。</p>
</li>
<li><p>attention：在机器学习和自然语言处理（NLP）的背景下，特别是在像 GPT 这样的生成预训练变换器（Generative Pre-trained Transformer）模型中，注意力机制是一种重要的技术，它允许模型在处理输入数据时动态地聚焦于信息的不同部分。<br>简单来说，注意力机制使模型能够为输入序列中的每个元素（如单词或字符）分配一个权重，这些权重表示在生成每个输出元素时，输入序列的哪些部分更加重要。这种机制使得模型能够捕捉到长距离依赖关系，并提高对上下文的理解能力。</p>
</li>
</ul>

        </div>
        
                <!-- tags -->
                
                    <div class="post-meta">
                        标签：
                        
                            <a href="/tags/%E9%98%85%E8%AF%BB/"> / 阅读</a>
                            
                    </div>
                    
</div>
                <div class="footer">
    <span>Copyright © 2024 fzero 的日常记录</span>
    <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https:///imzl.com/zenmind">ZenMind</a></span>
</div>

<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>

<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>